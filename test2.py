# Импортируем библиотеку для выполнения HTTP-запросов в интернет
import requests
from math import log

# Читаем текстовый файл по url-ссылке
data = requests.get("https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt").text

# Предобрабатываем текстовый файл
data = data.split('\n')
data.remove('')
data = data + ['[new chapter]']
# 
# 
# Превращаем список в множество, удаляя дублирующиеся слова
word_set = set(data)
# Удаляем из множества слово, символизирующее раздел между главами
word_set.discard('[new chapter]')
# Выводим результаты
# print('Общее количество слов: {}'.format(len(data)))
# print('Общее количество уникальных слов: {}'.format(len(word_set)))

# Инициализируем пустой словарь
word_counts = {}
# Инициализируем количество глав
count_chapter = 0
# Создаем цикл по всем словам из списка слов
for word in data:
    # Проверяем, что текущее слово - обозначение новой главы
    if word == '[new chapter]':
        # Если условие выполняется, то увеличиваем количество глав на 1
        count_chapter += 1
        # Переходим на новую итерацию цикла
        continue
    # Проверяем, что текущего слова еще нет в словаре слов
    if word not in word_counts:
        # Если условие выполняется, инициализируем новый ключ 1
        word_counts[word] = 1
    else:
        # В противном случае, увеличиваем количество слов на 1
        word_counts[word] += 1

# Выводим количество глав
# print('Количество глав: {}'.format(count_chapter))

# Создаем цикл по ключам и их порядковым номерам полученного словаря
'''for i, key in enumerate(word_counts):
    # Выводим только первые 10 слов
    if i == 10:
        break
    print(key, word_counts[key])'''

# Инициализируем общий список, в котором будем хранить списки слов в каждой главе
chapter_data = []
# Инициализируем список слов, в котором будет хранить слова одной главы
chapter_words = []

# Создаем цикл по всем словам из списка
for word in data:
    # Проверяем, что текущее слово - обозначение новой главы
    if word == '[new chapter]':
        # Если условие выполняется, добавляем список со словами из главы в общий список
        chapter_data.append(chapter_words)
        # Обновляем (перезаписываем) список со словами из текущей главы
        chapter_words = []
    else:
        # В противном случае, добавляем текущее слово в список со словами из главы
        chapter_words.append(word)

# Проверяем, что у нас получилось столько же списков, сколько глав в произведении
# print('Вложенный список содержит {} внутренних списка'.format(len(chapter_data)))с
# Выведем первые 100 слов 0-ой главы
# print(chapter_data[0][:100])

# Инициализируем список, в котором будем хранить словари
chapter_words_count = []

# Создаем цикл по элементам внешнего списка со словами
for chapter_words in chapter_data:
    # Инициализируем пустой словарь, куда будем добавлять результаты
    temp = {}
    # Создаем цикл по элементам внутреннего списка
    for word in chapter_words:
        # Проверяем, что текущего слова еще нет в словаре
        if word not in temp:
            # Если условие выполняется, добавляем ключ в словарь
            temp[word] = 1
        else:
            # В противном случае, увеличиваем количество влождений слова в главу
            temp[word] += 1
    # Добавляем получившийся словарь в список
    chapter_words_count.append(temp)

# Выводим результат
# print(chapter_words_count)

# target_word = 'гостья'
# target_chapter = 15

# Задание 1
'''
Давайте введем понятие частоты употребления отдельного слова в документе (term frequency, или tf).
В нашем случае речь идёт не о документах, а о главах книги
(выше мы писали, что в текстовом документе главы разделяются строкой '[new chapter]').
Формула для вычисления term frequency для слова word:
tf_word,chapter=n_word,chapter/n_chapter
где:
n_word,chapter  - сколько раз слово word встрачается в главе chapter
n_chapter  - количество слов в главе chapter
Например, слово "гостья" употребляется в 15-ой главе 10 раз (nword,chapter).
(кстати, главы у нас нумеруются с 0). Общее количество слов в тексте 15-ой главы - 1359 (nchapter). 
Тогда:
tf_гостья,15=10/1359≈0.007358

'''

# Задание:
'''
Напишите программу, которая позволит получать частоту употребления любого заданного слова target_word
в заданной главе target_chapter.

Дополнительное требование:

Пострайтесь сделать программу максимально обобщенной. То есть желательно 
рассчитать характеристику tf для всех слов из каждой главы, чтобы 
впоследствии не было необходимости производить вычисления снова.

Подсказка:

Для этого вы можете для каждой главы создать словарь, 
ключами которого являются слова, а значения - частота употребления этого слова в этой главе
'''
# Инициализируем список словарей для данных tf
tf_dict_list = []
# Создаём цикл по нашему списку со словарями chapter_words_count
for i, chapter in enumerate(chapter_words_count):
    # Создаём словарь ключами которого являются слова,
    # а значениями tf-частота употребления этого слова в текущей главе
    tf_dict = {}
    # Создаём цикл по главе
    for tf_word in chapter:
        # Вносим в словарь, слово и частоту употребления tf по формуле
        tf_dict[tf_word] = chapter[tf_word] / len(chapter_data[i])
    # Заносим в список наш словарь с tf-частотой слов
    tf_dict_list.append(tf_dict)

# Создаём цикл по списку словарей для отображения искомого слова и его частоты
'''for i, chap in enumerate(tf_dict_list):
    # Если индекс совпадает с искомым
    if i == target_chapter:
        # создаём цикл по главе
        for j in chap:
            # Если слово соответствует искомому
            if j == target_word:
                # Выводим на экран его
                # print(tf_dict_list[i][j])'''
                
# Задание 2
# Пришло время познакомиться с понятием document frequency.
'''
Document frequency (для удобства сократим до df) — это доля документов,
в которых встречается искомое слово.
Формула df:   df_word=N_word/N
где:
N_word  - число документов (глав) содержащих слово word
N  - общее число документов (глав)
Объясним на примере: 
наш текст состоит из 171 главы ( N ), а слово "человек" встречается в 115 главах.
Тогда: df_человек=115/171≈0.6725
'''

# Задание: 2
'''
Напишите программу, которая позволит вычислять document frequency для заданного слова target_word
и выведить результат на экран.
Дополнительное требование:
Пострайтесь сделать программу максимально обобщенной. То есть желательно
рассчитать характеристику df для всех уникальных слов из книги, чтобы впоследствии 
не было необходимости производить вычисления снова.
Подсказка: Для этого вы можете создать словарь, ключами которого являются слова из книги, 
а значения - доля документов, содержащих эти слова
'''
# Инициализируем словарь df
df_dict = {}
# Создаём цикл по ранее созданному списку со словарями(главами) 
for chapter in chapter_words_count:
    # Создаём цикл по словам из итегого словаря
    for word in chapter:
        # Если слова нету в новом словаре
        if word not in df_dict:
            # Заносим с словарь это слово со значением в виде формулы вычисления df
            df_dict[word] = 1 / count_chapter
        # В противном случае(если итовое слово там уже есть)
        else:
            # Добавляем к значению этого слова формулу вычисления df
            df_dict[word] += 1 / count_chapter
# Тестовая программа для проверки поиска определённого слова в определённой главе
target_word = 'человек'
print(df_dict[target_word])
# Создаём цикл по словарю df
'''for word in df_dict:
    # Если итовое слово является искомым словом
    if word == target_word:
    # Выводим на экран df значение этого слова
        print(df_dict[target_word])'''

# Задание 3
# Пришло время дать разъяснения: для чего мы делали вычисления выше и что нас ждет впереди?
'''
Если какое-то слово часто употребляется в документе, то, вероятно, этот документ что-то 
рассказывает о предмете/действии, описываемом этим словом. Скажем, если вы читаете книгу, 
в которой много раз употребляется слово "заяц", то, вероятно, эта книга про зайцев.

Однако, если вы возьмёте слово "и", то оно будет встречаться почти в каждой книге много раз.

Таким образом, если мы хотим найти наиболее значимые слова в книге, мы, с одной стороны,
хотим найти наиболее частые слова, а с другой — убрать те, которые не несут важной информации, 
так как встречаются везде.

Такая задача хорошо решается с помощью 
tf-idf — статистической метрики для оценки важности слова в тексте. Другими словами, 
tf-idf — это «контрастность» слова в документе (насколько оно выделяется среди других слов).

Формула для вычисления следующая:

tf-idf = term frequency * inverse document frequency

tf — это частотность термина, которая измеряет, насколько часто термин встречается в документе.

idf — это обратная документная частотность термина. 
Она измеряет непосредственно важность термина во всём множестве документов.

Чтобы получить idf, необходимо поделить 1 на полученную в Задании 2 документную частоту (df):

idf=1/df 

Мы будем использовать не сырые значения idf, а их логарифмы, то есть  tf∗log(idf) . 
Сейчас мы не будем заострять внимания на том, почему следует использовать именно логарифм 
— это долгий разговор. Вернемся к нему, когда будем изучать методы 
машинного обучения для обработки текстов. Подробнее о tf-idf вы можете почитать здесь.

В качестве примера измерим tf-idf слова "анна" в главе 4. Слово "анна" встречается 
в указанной главе 7 раз, при этом в 4 главе 1060 слов, всего же 
слово "анна" упоминается в 32 главах из 171.

Таким образом, tf-idf данного слова в данной главе будет равно:

tf_idfанна,4=tf∗log(1df)=71060∗log(17132)≈0.011067 

Примечание: здесь используется натуральный логарифм по основанию  e , 
однако в общем случае основание логарифма не имеет значения, 
так как характеристика tf-idf используется для сравнения контрастности слов между собой
'''

# Создаём список словарей со значениями tf-idf для каждого слова в главе
tf_idf_dict_list = []
# Создаём цикл по индексам и главам предыдущего списка со словарями данных tf
for i, chap in enumerate(tf_dict_list):
    # Создаём словарь tf-idf для каждого слова в этой главе
    tf_idf_dict = {}
    # Создаём цикл по словам в итовой главе
    for word in chap:
        # Заносим в новый словарь слово и присваеваем ему значение по формуле: tf*log(1/df)
        tf_idf_dict[word] = tf_dict_list[i][word]*log(1/df_dict[word])
    # Заносим в наш список получившийся словарь 
    tf_idf_dict_list.append(tf_idf_dict)

# target_word = 'анна'
# target_chapter = 4
# Программа для поиска определённого слова в определённой главе
'''for i, chap in enumerate(tf_dict_list):
    # Если индекс совпадает с искомым
    if i == target_chapter:
        # создаём цикл по главе
        for j in chap:
            # Если слово соответствует искомому
            if j == target_word:
                # Применяем к нему формулу расчёта tf-idf и выводим на экран 
                print(tf_dict_list[i][j]*log(1/df_dict[j]))'''

# Задание 4
'''
Теперь, когда мы умеем вычислять tf-idf для каждого слова в главе, мы можем найти те слова, 
которые являются самыми «контрастными» для данной главы, то есть они 
могут являться в своём роде заголовком для главы.

Например, для главы 3 наиболее значимыми словами будут:

"анна", "павловна", "функе"

Задание:

Напишите программу, которая позволяет вывести три слова, имеющие 
самое высокое значение tf-idf в заданной главе target_chapter в порядке убывания tf-idf.
'''

target_chapter = 3
# Используем функцию сортировки sorted
# Которая на вход получает словарь заданной главы, а ключом для сортировки служат значения этого словаря, полученные через метод get
# В переменную записывается список отсортированных ключей
sorted_keys = sorted(
    tf_idf_dict_list[target_chapter],
    key=tf_idf_dict_list[target_chapter].get,
    reverse=True)

# Можем сразу вывести первые 3 контрастных слова 
print(sorted_keys[:3])

# это мой предыдущий вариант
'''
# target_chapter = 3
# Создаём цикл по нашему списку словарей tf-idf с использованием индексов по главам
for i, chapter in enumerate(tf_idf_dict_list):
    # Если индекс главы равен искомому
    if i == target_chapter:
        # Создаём пустой словарь для сортировки
        sorted_dict = {}
        # Создаём переменную с отсортироваными ключами по убыванию (возвращает в виде списка)
        sorted_keys = sorted(chapter, key=chapter.get, reverse=True)
        # Создаём цикл по списку отсортированых ключей
        for w in sorted_keys:
            # Заносим в словарь ключ и присваеваем ему значение этого ключа из словаря(главы) chapter
            sorted_dict[w] = chapter[w]
        # Создаём цикл по индексам и элементам отсортированого словаря
        for j, slovo in enumerate(sorted_dict):
            # Выводим только первые 3 слова
            if j == 3:
                break
            print(slovo,sorted_dict[slovo])
'''